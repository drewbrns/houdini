{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from pattern.vector import count, words\n",
    "from pattern.vector import stem, PORTER, LEMMA\n",
    "from pattern.vector import Document, Model, TFIDF, IG, BINARY\n",
    "from pattern.vector import Vector, distance, tfidf\n",
    "\n",
    "from pattern.en import parse, Sentence, parsetree\n",
    "from pattern.en import wordnet, NOUN, VERB, ADJECTIVE, ADVERB\n",
    "from pattern.en import sentiment\n",
    "import pattern.en\n",
    "\n",
    "from pandas import Series, DataFrame\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import itertools\n",
    "\n",
    "# from nltk.corpus import stopwords\n",
    "# from nltk.corpus import wordnet as wn\n",
    "\n",
    "from textblob import TextBlob\n",
    "# from textblob.wordnet import VERB, Synset\n",
    "\n",
    "PA_WORDS  = ['buy', 'recommend', 'hire', 'have', 'suggest', 'advise', 'want', 'need', 'purchase', 'wish', 'pay']\n",
    "VERB_TAGS = ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
    "NOUN_TAGS = ['NN', 'NNS', 'NNP', 'NNPS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "  {\"text\": \"I need silence, and to be alone and to go out, and to save one hour…\", \"class\": \"non-pi\"},\n",
    "  {\"text\": \"I need more tiaras. I haven't bought a new one in over a year and that's a shame. Send an Amazon gift card so I can fix this today! goddessjoules@gmail.com\", \"class\": \"pi\"},  \n",
    "  {\"text\": \"Hi, recently I was wondering if it pays to buy gtx 1070 or wait for new this year's nvidia graphics cards\", \"class\": \"pi\"},\n",
    "  {\"text\": \"I have been planning for years to immigrate to Canada, the heaven of justice! and now it’s over a year/years that most of #OutlandIranianApplicants are stuck in the assessment limbo of @CitImmCanada. @AhmedDHussen @JustinTrudeau @RalphGoodale @ICCongress\", \"class\": \"non-pi\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = wordnet.synsets(\"go\", pos=VERB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('travel.v.01'),\n",
       " Synset('go.v.02'),\n",
       " Synset('go.v.03'),\n",
       " Synset('become.v.01'),\n",
       " Synset('go.v.05'),\n",
       " Synset('run.v.05'),\n",
       " Synset('run.v.03'),\n",
       " Synset('proceed.v.04'),\n",
       " Synset('go.v.09'),\n",
       " Synset('go.v.10'),\n",
       " Synset('sound.v.02'),\n",
       " Synset('function.v.01'),\n",
       " Synset('run_low.v.01'),\n",
       " Synset('move.v.13'),\n",
       " Synset('survive.v.01'),\n",
       " Synset('go.v.16'),\n",
       " Synset('die.v.01'),\n",
       " Synset('belong.v.03'),\n",
       " Synset('go.v.19'),\n",
       " Synset('start.v.09'),\n",
       " Synset('move.v.15'),\n",
       " Synset('go.v.22'),\n",
       " Synset('go.v.23'),\n",
       " Synset('blend.v.02'),\n",
       " Synset('go.v.25'),\n",
       " Synset('fit.v.02'),\n",
       " Synset('rifle.v.02'),\n",
       " Synset('go.v.28'),\n",
       " Synset('plump.v.04'),\n",
       " Synset('fail.v.04')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('travel.v.01')  means ... change location; move, travel, or proceed, also metaphorically\n",
      "Synset('go.v.02')  means ... follow a procedure or take a course\n",
      "Synset('go.v.03')  means ... move away from a place into another direction\n",
      "Synset('become.v.01')  means ... enter or assume a certain state or condition\n",
      "Synset('go.v.05')  means ... be awarded; be allotted\n",
      "Synset('run.v.05')  means ... have a particular form\n",
      "Synset('run.v.03')  means ... stretch out over a distance, space, time, or scope; run or extend between two points or beyond a certain point\n",
      "Synset('proceed.v.04')  means ... follow a certain course\n",
      "Synset('go.v.09')  means ... be abolished or discarded\n",
      "Synset('go.v.10')  means ... be or continue to be in a certain condition\n",
      "Synset('sound.v.02')  means ... make a certain noise or sound\n",
      "Synset('function.v.01')  means ... perform as expected when applied\n",
      "Synset('run_low.v.01')  means ... to be spent or finished\n",
      "Synset('move.v.13')  means ... progress by being changed\n",
      "Synset('survive.v.01')  means ... continue to live through hardship or adversity\n",
      "Synset('go.v.16')  means ... pass, fare, or elapse; of a certain state of affairs or action\n",
      "Synset('die.v.01')  means ... pass from physical life and lose all bodily attributes and functions necessary to sustain life\n",
      "Synset('belong.v.03')  means ... be in the right place or situation\n",
      "Synset('go.v.19')  means ... be ranked or compare\n",
      "Synset('start.v.09')  means ... begin or set in motion\n",
      "Synset('move.v.15')  means ... have a turn; make one's move in a game\n",
      "Synset('go.v.22')  means ... be contained in\n",
      "Synset('go.v.23')  means ... be sounded, played, or expressed\n",
      "Synset('blend.v.02')  means ... blend or harmonize\n",
      "Synset('go.v.25')  means ... lead, extend, or afford access\n",
      "Synset('fit.v.02')  means ... be the right size or shape; fit correctly or as desired\n",
      "Synset('rifle.v.02')  means ... go through in search of something; search through someone's belongings in an unauthorized way\n",
      "Synset('go.v.28')  means ... be spent\n",
      "Synset('plump.v.04')  means ... give support (to) or make a choice (of) one out of a group or number\n",
      "Synset('fail.v.04')  means ... stop operating or functioning\n"
     ]
    }
   ],
   "source": [
    "for x in s:\n",
    "    print(x, \" means ...\", x.gloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = Document(data[0]['text'], type=data[0]['class'], stemmer=LEMMA)\n",
    "d2 = Document(data[1]['text'], type=data[1]['class'], stemmer=LEMMA)\n",
    "d3 = Document(data[2]['text'], type=data[2]['class'], stemmer=LEMMA)\n",
    "d4 = Document(data[3]['text'], type=data[3]['class'], stemmer=LEMMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Model(documents=[d1, d2, d3, d4], weight=TFIDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'silence': 1.3862952936151878},\n",
       " {'tiara': 0.1540328104016875,\n",
       "  't': 0.1540328104016875,\n",
       "  'bought': 0.1540328104016875,\n",
       "  'shame': 0.1540328104016875,\n",
       "  'send': 0.1540328104016875,\n",
       "  'amazon': 0.1540328104016875,\n",
       "  'gift': 0.1540328104016875,\n",
       "  'card': 0.07701640520084375,\n",
       "  'fix': 0.1540328104016875},\n",
       " {'hi': 0.12602684487410798,\n",
       "  'recently': 0.12602684487410798,\n",
       "  'wondering': 0.12602684487410798,\n",
       "  'pay': 0.12602684487410798,\n",
       "  'buy': 0.12602684487410798,\n",
       "  'gtx': 0.12602684487410798,\n",
       "  '1070': 0.12602684487410798,\n",
       "  'wait': 0.12602684487410798,\n",
       "  'nvidium': 0.12602684487410798,\n",
       "  'graphic': 0.12602684487410798,\n",
       "  'card': 0.06301342243705399},\n",
       " {'planning': 0.09902109240108484,\n",
       "  'immigrate': 0.09902109240108484,\n",
       "  'canada': 0.09902109240108484,\n",
       "  'heaven': 0.09902109240108484,\n",
       "  'justice': 0.09902109240108484,\n",
       "  'outlandiranianapplicant': 0.09902109240108484,\n",
       "  'stuck': 0.09902109240108484,\n",
       "  'assessment': 0.09902109240108484,\n",
       "  'limbo': 0.09902109240108484,\n",
       "  'citimmcanada': 0.09902109240108484,\n",
       "  'ahmeddhussen': 0.09902109240108484,\n",
       "  'justintrudeau': 0.09902109240108484,\n",
       "  'ralphgoodale': 0.09902109240108484,\n",
       "  'iccongres': 0.09902109240108484}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['non-pi', 'pi']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tf-idf'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.df('buy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3862952936151878"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.idf('buy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.similarity(d1, d2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.similarity(d1, d3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.similarity(d1, d4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.027186362391351835"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.similarity(d2, d3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.similarity(d2, d4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.similarity(d3, d4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hi': 0.12602684487410798,\n",
       " 'recently': 0.12602684487410798,\n",
       " 'wondering': 0.12602684487410798,\n",
       " 'pay': 0.12602684487410798,\n",
       " 'buy': 0.12602684487410798,\n",
       " 'gtx': 0.12602684487410798,\n",
       " '1070': 0.12602684487410798,\n",
       " 'wait': 0.12602684487410798,\n",
       " 'nvidium': 0.12602684487410798,\n",
       " 'graphic': 0.12602684487410798,\n",
       " 'card': 0.06301342243705399}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d3.vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'silence': 1.9218146410996197},\n",
       " {'tiara': 0.21353496012217993,\n",
       "  't': 0.21353496012217993,\n",
       "  'bought': 0.21353496012217993,\n",
       "  'shame': 0.21353496012217993,\n",
       "  'send': 0.21353496012217993,\n",
       "  'amazon': 0.21353496012217993,\n",
       "  'gift': 0.21353496012217993,\n",
       "  'card': 0.05338374003054498,\n",
       "  'fix': 0.21353496012217993},\n",
       " {'hi': 0.17471042191814726,\n",
       "  'recently': 0.17471042191814726,\n",
       "  'wondering': 0.17471042191814726,\n",
       "  'pay': 0.17471042191814726,\n",
       "  'buy': 0.17471042191814726,\n",
       "  'gtx': 0.17471042191814726,\n",
       "  '1070': 0.17471042191814726,\n",
       "  'wait': 0.17471042191814726,\n",
       "  'nvidium': 0.17471042191814726,\n",
       "  'graphic': 0.17471042191814726,\n",
       "  'card': 0.043677605479536814},\n",
       " {'planning': 0.13727247436425855,\n",
       "  'immigrate': 0.13727247436425855,\n",
       "  'canada': 0.13727247436425855,\n",
       "  'heaven': 0.13727247436425855,\n",
       "  'justice': 0.13727247436425855,\n",
       "  'outlandiranianapplicant': 0.13727247436425855,\n",
       "  'stuck': 0.13727247436425855,\n",
       "  'assessment': 0.13727247436425855,\n",
       "  'limbo': 0.13727247436425855,\n",
       "  'citimmcanada': 0.13727247436425855,\n",
       "  'ahmeddhussen': 0.13727247436425855,\n",
       "  'justintrudeau': 0.13727247436425855,\n",
       "  'ralphgoodale': 0.13727247436425855,\n",
       "  'iccongres': 0.13727247436425855}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf([d1.vector, d2.vector, d3.vector, d4.vector])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(word_list):\n",
    "    if not isinstance(word_list, list):\n",
    "        raise (TypeError, '`word_list` argument must be a list')\n",
    "    return [\n",
    "        word for word in word_list if word not in set(stopwords.words('english'))\n",
    "        ]\n",
    "\n",
    "def extract_pos(blob, POS_TAGS):\n",
    "    if not isinstance(blob, TextBlob):\n",
    "        raise (TypeError, '`blob` argument must be a TextBlob')\n",
    "    if not isinstance(POS_TAGS, list):\n",
    "        raise (TypeError, '`POS_TAGS` argument must be a list')        \n",
    "    return filter(lambda x: x[1] in POS_TAGS, blob.tags)\n",
    "\n",
    "def extract_terms(blob, pos):\n",
    "    if pos.lower() == 'verb' or pos.lower() == 'vb':\n",
    "        verbs = extract_pos(blob, VERB_TAGS)\n",
    "        return [x[0] for x in verbs]\n",
    "    elif pos.lower() == 'noun' or pos.lower() == 'nn':\n",
    "        nouns = extract_pos(blob, NOUN_TAGS)\n",
    "        return [x[0] for x in nouns]\n",
    "    elif pos.lower() == 'pronoun' or pos.lower() == 'pn':\n",
    "        raise(Exception('Not implemented'))\n",
    "    elif pos.lower() == 'adjective' or pos.lower() == 'adj':\n",
    "        raise(Exception('Not implemented'))\n",
    "    elif pos.lower() == 'adverb' or pos.lower() == 'adv':\n",
    "        raise(Exception('Not implemented'))\n",
    "    elif pos.lower() == 'preposition' or pos.lower() == 'pre':\n",
    "        raise(Exception('Not implemented'))\n",
    "    elif pos.lower() == 'conjunction' or pos.lower() == 'cn':\n",
    "        raise(Exception('Not implemented'))\n",
    "    elif pos.lower() == 'interjection' or pos.lower() == 'in':\n",
    "        raise(Exception('Not implemented'))\n",
    "    else:\n",
    "        raise(Exception('Please provide valid pos. verb, noun, pronoun, adjective')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "b3 = TextBlob(data[2]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"Hi, recently I was wondering if it pays to buy gtx 1070 or wait for new this year's nvidia graphics cards\")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbs = extract_terms(b3, 'verb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['was', 'wondering', 'pays', 'buy', 'wait']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = wordnet.synsets('bird')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Synset('bird.n.01')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('bird.n.01'),\n",
       " Synset('bird.n.02'),\n",
       " Synset('dame.n.01'),\n",
       " Synset('boo.n.01'),\n",
       " Synset('shuttlecock.n.01')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bird_synsets = wordnet.synsets('bird')\n",
    "bird_synsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "synset_1 = wordnet.synsets('buy', pos=VERB)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "synset_2 = wordnet.synsets('buy', pos=VERB)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'obtain by purchase; acquire by means of a financial transaction'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synset_1.gloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['buy', 'purchase']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synset_1.synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synset_1.similarity(synset_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06818181818181818"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment(data[2]['text'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc3 = data[2]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed = parse(doc3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['word', 'part-of-speech', 'chunk', 'preposition']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed.tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hi, recently I was wondering if it pays to buy gtx 1070 or wait for new this year's nvidia graphics cards\""
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['Hi', 'UH', 'O', 'O'],\n",
       "  [',', ',', 'O', 'O'],\n",
       "  ['recently', 'RB', 'B-ADVP', 'O'],\n",
       "  ['I', 'PRP', 'B-NP', 'O'],\n",
       "  ['was', 'VBD', 'B-VP', 'O'],\n",
       "  ['wondering', 'VBG', 'I-VP', 'O'],\n",
       "  ['if', 'IN', 'B-PP', 'B-PNP'],\n",
       "  ['it', 'PRP', 'B-NP', 'I-PNP'],\n",
       "  ['pays', 'VBZ', 'B-VP', 'O'],\n",
       "  ['to', 'TO', 'I-VP', 'O'],\n",
       "  ['buy', 'VB', 'I-VP', 'O'],\n",
       "  ['gtx', 'NN', 'B-NP', 'O'],\n",
       "  ['1070', 'CD', 'I-NP', 'O'],\n",
       "  ['or', 'CC', 'I-NP', 'O'],\n",
       "  ['wait', 'NN', 'I-NP', 'O'],\n",
       "  ['for', 'IN', 'B-PP', 'O'],\n",
       "  ['new', 'JJ', 'B-ADJP', 'O'],\n",
       "  ['this', 'DT', 'B-NP', 'O'],\n",
       "  ['year', 'NN', 'I-NP', 'O'],\n",
       "  [\"'s\", 'POS', 'O', 'O'],\n",
       "  ['nvidia', 'NN', 'B-NP', 'O'],\n",
       "  ['graphics', 'NNS', 'I-NP', 'O'],\n",
       "  ['cards', 'NNS', 'I-NP', 'O']]]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse(doc3).split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = parsetree(doc3, relations=True, lemmata=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = s[0].chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "String : recently\n",
      "Type   : ADVP\n",
      "Role   : None\n",
      "Related: []\n",
      "Head   : []\n",
      "\n",
      "String : I\n",
      "Type   : NP\n",
      "Role   : SBJ\n",
      "Related: [Chunk('was wondering/VP-1')]\n",
      "Head   : [Chunk('was wondering/VP-1')]\n",
      "\n",
      "String : was wondering\n",
      "Type   : VP\n",
      "Role   : None\n",
      "Related: [Chunk('I/NP-SBJ-1')]\n",
      "Head   : [Chunk('I/NP-SBJ-1')]\n",
      "\n",
      "String : if\n",
      "Type   : PP\n",
      "Role   : None\n",
      "Related: []\n",
      "Head   : []\n",
      "\n",
      "String : it\n",
      "Type   : NP\n",
      "Role   : SBJ\n",
      "Related: [Chunk('pays to buy/VP-2'), Chunk('gtx 1070 or wait/NP-OBJ-2')]\n",
      "Head   : [Chunk('pays to buy/VP-2'), Chunk('gtx 1070 or wait/NP-OBJ-2')]\n",
      "\n",
      "String : pays to buy\n",
      "Type   : VP\n",
      "Role   : None\n",
      "Related: [Chunk('it/NP-SBJ-2'), Chunk('gtx 1070 or wait/NP-OBJ-2')]\n",
      "Head   : [Chunk('it/NP-SBJ-2'), Chunk('gtx 1070 or wait/NP-OBJ-2')]\n",
      "\n",
      "String : gtx 1070 or wait\n",
      "Type   : NP\n",
      "Role   : OBJ\n",
      "Related: [Chunk('it/NP-SBJ-2'), Chunk('pays to buy/VP-2')]\n",
      "Head   : [Chunk('it/NP-SBJ-2'), Chunk('pays to buy/VP-2')]\n",
      "\n",
      "String : for\n",
      "Type   : PP\n",
      "Role   : None\n",
      "Related: []\n",
      "Head   : []\n",
      "\n",
      "String : new\n",
      "Type   : ADJP\n",
      "Role   : None\n",
      "Related: []\n",
      "Head   : []\n",
      "\n",
      "String : this year\n",
      "Type   : NP\n",
      "Role   : None\n",
      "Related: []\n",
      "Head   : []\n",
      "\n",
      "String : nvidia graphics cards\n",
      "Type   : NP\n",
      "Role   : None\n",
      "Related: []\n",
      "Head   : []\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for chunk in chunks:\n",
    "    print(\"String :\", chunk.string)\n",
    "    print(\"Type   :\", chunk.type)\n",
    "    print(\"Role   :\", chunk.role)\n",
    "    print(\"Related:\", chunk.related)\n",
    "    print(\"Head   :\", chunk.related)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_ado(document):\n",
    "    s = parsetree(document, relations=True, lemmata=True)\n",
    "    for sent in s:\n",
    "        for chunk in sent.chunks:\n",
    "            if chunk.type == 'VP':\n",
    "                print(\"Subject          : \", chunk.subject)            \n",
    "                print(\"[VP]             : \", chunk.string)\n",
    "                print(\"[VP] [WORDS]     : \", chunk.words)   \n",
    "                print(\"[VP] [HEAD]      : \", chunk.head)\n",
    "                if chunk.object is not None:\n",
    "                    print(\"Object [WORDS]   : \", chunk.object.words)\n",
    "                    print(\"Object [HEAD]    : \", chunk.object.head)\n",
    "                    nouns = filter(lambda x: x.type in NOUN_TAGS, chunk.object.words)        \n",
    "                    nouns = set([noun.string for noun in nouns])\n",
    "                    print(\"Extracted Nouns  :\", nouns)\n",
    "                else:\n",
    "                    print(\"Object           : \", chunk.object)            \n",
    "                print('-'*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_verbs(document):        \n",
    "    blob = TextBlob(document)        \n",
    "    tags = filter(lambda x: x[1] in VERB_TAGS, blob.tags)            \n",
    "    return [verb[0] for verb in tags]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['was', 'wondering', 'pays', 'buy', 'wait']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_get_verbs(doc3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_pa_similarity( word ):\n",
    "    s = wordnet.synsets(word, pos=VERB)[0]\n",
    "\n",
    "    similarities = []\n",
    "    append_sim = similarities.append\n",
    "\n",
    "    for paword in PA_WORDS:\n",
    "        pa_s = wordnet.synsets(paword, pos=VERB)[0]\n",
    "        append_sim( s.similarity(pa_s) )\n",
    "\n",
    "    print(similarities)\n",
    "    return max(similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_get_pa_similarity('buy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_get_pa_similarity('purchase')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_get_pa_similarity('pay')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5654954205996426, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5654954205996426, 0.0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5654954205996426"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_get_pa_similarity('obtain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('obtain.v.01'), Synset('receive.v.02'), Synset('prevail.v.02')]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet.synsets('obtain', pos=VERB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_verbs = _get_verbs(doc3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_rows = len(extracted_verbs)\n",
    "count_columns = len(PA_WORDS)\n",
    "frame = DataFrame(np.empty((count_rows, count_columns)), index=extracted_verbs, columns=PA_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>buy</th>\n",
       "      <th>recommend</th>\n",
       "      <th>hire</th>\n",
       "      <th>have</th>\n",
       "      <th>suggest</th>\n",
       "      <th>advise</th>\n",
       "      <th>want</th>\n",
       "      <th>need</th>\n",
       "      <th>purchase</th>\n",
       "      <th>wish</th>\n",
       "      <th>pay</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>was</th>\n",
       "      <td>6.931999e-310</td>\n",
       "      <td>1.122473e-316</td>\n",
       "      <td>4.207069e-310</td>\n",
       "      <td>7.764465e+69</td>\n",
       "      <td>1.685826e-312</td>\n",
       "      <td>2.561371e-300</td>\n",
       "      <td>1.333605e+241</td>\n",
       "      <td>1.735326e-309</td>\n",
       "      <td>1.807805e+60</td>\n",
       "      <td>1.855586e-312</td>\n",
       "      <td>5.216755e-307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wondering</th>\n",
       "      <td>-3.384608e+125</td>\n",
       "      <td>3.690289e+180</td>\n",
       "      <td>1.199333e-302</td>\n",
       "      <td>1.094038e-303</td>\n",
       "      <td>1.433734e-298</td>\n",
       "      <td>1.184992e-303</td>\n",
       "      <td>1.823153e-303</td>\n",
       "      <td>1.184994e-303</td>\n",
       "      <td>1.093850e-303</td>\n",
       "      <td>1.002710e-303</td>\n",
       "      <td>8.204348e-304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pays</th>\n",
       "      <td>4.681025e-188</td>\n",
       "      <td>5.997930e+19</td>\n",
       "      <td>2.710567e-24</td>\n",
       "      <td>3.796058e+77</td>\n",
       "      <td>1.634523e+87</td>\n",
       "      <td>2.657113e+82</td>\n",
       "      <td>3.212263e+106</td>\n",
       "      <td>5.038551e-111</td>\n",
       "      <td>4.720251e+154</td>\n",
       "      <td>9.249271e-304</td>\n",
       "      <td>9.251051e-304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>buy</th>\n",
       "      <td>1.250207e+63</td>\n",
       "      <td>8.298721e-304</td>\n",
       "      <td>1.272151e-178</td>\n",
       "      <td>5.516117e-294</td>\n",
       "      <td>4.262554e+24</td>\n",
       "      <td>9.714609e+303</td>\n",
       "      <td>3.738515e-217</td>\n",
       "      <td>5.545157e-294</td>\n",
       "      <td>9.604798e+303</td>\n",
       "      <td>3.770047e-265</td>\n",
       "      <td>3.594492e-289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wait</th>\n",
       "      <td>1.093077e+145</td>\n",
       "      <td>8.245237e-304</td>\n",
       "      <td>9.115710e-304</td>\n",
       "      <td>7.168650e-299</td>\n",
       "      <td>5.567341e-294</td>\n",
       "      <td>3.882846e+130</td>\n",
       "      <td>5.633431e+255</td>\n",
       "      <td>1.391582e+116</td>\n",
       "      <td>3.247651e+106</td>\n",
       "      <td>3.311374e-183</td>\n",
       "      <td>2.213414e-321</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     buy      recommend           hire           have  \\\n",
       "was        6.931999e-310  1.122473e-316  4.207069e-310   7.764465e+69   \n",
       "wondering -3.384608e+125  3.690289e+180  1.199333e-302  1.094038e-303   \n",
       "pays       4.681025e-188   5.997930e+19   2.710567e-24   3.796058e+77   \n",
       "buy         1.250207e+63  8.298721e-304  1.272151e-178  5.516117e-294   \n",
       "wait       1.093077e+145  8.245237e-304  9.115710e-304  7.168650e-299   \n",
       "\n",
       "                 suggest         advise           want           need  \\\n",
       "was        1.685826e-312  2.561371e-300  1.333605e+241  1.735326e-309   \n",
       "wondering  1.433734e-298  1.184992e-303  1.823153e-303  1.184994e-303   \n",
       "pays        1.634523e+87   2.657113e+82  3.212263e+106  5.038551e-111   \n",
       "buy         4.262554e+24  9.714609e+303  3.738515e-217  5.545157e-294   \n",
       "wait       5.567341e-294  3.882846e+130  5.633431e+255  1.391582e+116   \n",
       "\n",
       "                purchase           wish            pay  \n",
       "was         1.807805e+60  1.855586e-312  5.216755e-307  \n",
       "wondering  1.093850e-303  1.002710e-303  8.204348e-304  \n",
       "pays       4.720251e+154  9.249271e-304  9.251051e-304  \n",
       "buy        9.604798e+303  3.770047e-265  3.594492e-289  \n",
       "wait       3.247651e+106  3.311374e-183  2.213414e-321  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "for verb in extracted_verbs:\n",
    "    s = wordnet.synsets(verb, pos=VERB)[0]               \n",
    "    for paword in PA_WORDS:\n",
    "        pa_s = wordnet.synsets(paword, pos=VERB)[0]                \n",
    "        frame[paword][verb] = s.similarity(pa_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>buy</th>\n",
       "      <th>recommend</th>\n",
       "      <th>hire</th>\n",
       "      <th>have</th>\n",
       "      <th>suggest</th>\n",
       "      <th>advise</th>\n",
       "      <th>want</th>\n",
       "      <th>need</th>\n",
       "      <th>purchase</th>\n",
       "      <th>wish</th>\n",
       "      <th>pay</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>was</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wondering</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.360471</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pays</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>buy</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wait</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           buy  recommend  hire  have  suggest    advise  want  need  \\\n",
       "was        0.0        0.0   0.0   0.0      0.0  0.000000   0.0   0.0   \n",
       "wondering  0.0        0.0   0.0   0.0      0.0  0.360471   0.0   0.0   \n",
       "pays       0.0        0.0   0.0   0.0      0.0  0.000000   0.0   0.0   \n",
       "buy        1.0        0.0   0.0   0.0      0.0  0.000000   0.0   0.0   \n",
       "wait       0.0        0.0   0.0   0.0      0.0  0.000000   0.0   0.0   \n",
       "\n",
       "           purchase  wish  pay  \n",
       "was             0.0   0.0  0.0  \n",
       "wondering       0.0   0.0  0.0  \n",
       "pays            0.0   0.0  1.0  \n",
       "buy             1.0   0.0  0.0  \n",
       "wait            0.0   0.0  0.0  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "wpa_max_columns = frame.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "buy          1.000000\n",
       "recommend    0.000000\n",
       "hire         0.000000\n",
       "have         0.000000\n",
       "suggest      0.000000\n",
       "advise       0.360471\n",
       "want         0.000000\n",
       "need         0.000000\n",
       "purchase     1.000000\n",
       "wish         0.000000\n",
       "pay          1.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wpa_max_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7, 0.6000000000000001)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment('What are some good places to have lunch in Palo Alto?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "for i in range(8):\n",
    "    if i == 3:\n",
    "        break\n",
    "    else:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc5 = \"I want to purchase a new phone, should i go for the iphone x or google pixel?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject          :  I\n",
      "[VP]             :  was wondering\n",
      "[VP] [WORDS]     :  [Word('was/VBD'), Word('wondering/VBG')]\n",
      "[VP] [HEAD]      :  wondering\n",
      "Object           :  None\n",
      "--------------------------------------------------------------------------------\n",
      "Subject          :  it\n",
      "[VP]             :  pays to buy\n",
      "[VP] [WORDS]     :  [Word('pays/VBZ'), Word('to/TO'), Word('buy/VB')]\n",
      "[VP] [HEAD]      :  buy\n",
      "Object [WORDS]   :  [Word('gtx/NN'), Word('1070/CD'), Word('or/CC'), Word('wait/NN')]\n",
      "Object [HEAD]    :  wait\n",
      "Extracted Nouns  : {'wait', 'gtx'}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "find_ado(data[2]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject          :  I\n",
      "[VP]             :  want to purchase\n",
      "[VP] [WORDS]     :  [Word('want/VBP'), Word('to/TO'), Word('purchase/VB')]\n",
      "[VP] [HEAD]      :  purchase\n",
      "Object [WORDS]   :  [Word('a/DT'), Word('new/JJ'), Word('phone/NN')]\n",
      "Object [HEAD]    :  phone\n",
      "Extracted Nouns  : {'phone'}\n",
      "--------------------------------------------------------------------------------\n",
      "Subject          :  None\n",
      "[VP]             :  should i go\n",
      "[VP] [WORDS]     :  [Word('should/MD'), Word('i/VB'), Word('go/VB')]\n",
      "[VP] [HEAD]      :  go\n",
      "Object           :  None\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "find_ado(doc5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject          :  I\n",
      "[VP]             :  have been planning\n",
      "[VP] [WORDS]     :  [Word('have/VBP'), Word('been/VBN'), Word('planning/VBG')]\n",
      "[VP] [HEAD]      :  planning\n",
      "Object           :  None\n",
      "--------------------------------------------------------------------------------\n",
      "Subject          :  years\n",
      "[VP]             :  to immigrate\n",
      "[VP] [WORDS]     :  [Word('to/TO'), Word('immigrate/VB')]\n",
      "[VP] [HEAD]      :  immigrate\n",
      "Object           :  None\n",
      "--------------------------------------------------------------------------------\n",
      "Subject          :  OutlandIranianApplicants\n",
      "[VP]             :  are stuck\n",
      "[VP] [WORDS]     :  [Word('are/VBP'), Word('stuck/VBN')]\n",
      "[VP] [HEAD]      :  stuck\n",
      "Object           :  None\n",
      "--------------------------------------------------------------------------------\n",
      "Subject          :  None\n",
      "[VP]             :  @AhmedDHussen\n",
      "[VP] [WORDS]     :  [Word('@AhmedDHussen/VBN')]\n",
      "[VP] [HEAD]      :  @AhmedDHussen\n",
      "Object [WORDS]   :  [Word('@JustinTrudeau/NN'), Word('@RalphGoodale/NN'), Word('@ICCongress/NN')]\n",
      "Object [HEAD]    :  @ICCongress\n",
      "Extracted Nouns  : {'@RalphGoodale', '@ICCongress', '@JustinTrudeau'}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "find_ado(data[3]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I have been planning for years to immigrate to Canada, the heaven of justice! and now it’s over a year/years that most of #OutlandIranianApplicants are stuck in the assessment limbo of @CitImmCanada. @AhmedDHussen @JustinTrudeau @RalphGoodale @ICCongress'"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[3]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject          :  I\n",
      "[VP]             :  need\n",
      "[VP] [WORDS]     :  [Word('need/VBP')]\n",
      "[VP] [HEAD]      :  need\n",
      "Object [WORDS]   :  [Word('more/JJR'), Word('tiaras/NNS')]\n",
      "Object [HEAD]    :  tiaras\n",
      "Extracted Nouns  : {'tiaras'}\n",
      "--------------------------------------------------------------------------------\n",
      "Subject          :  I\n",
      "[VP]             :  have n't bought\n",
      "[VP] [WORDS]     :  [Word('have/VBP'), Word(\"n't/RB\"), Word('bought/VBN')]\n",
      "[VP] [HEAD]      :  bought\n",
      "Object           :  None\n",
      "--------------------------------------------------------------------------------\n",
      "Subject          :  None\n",
      "[VP]             :  's\n",
      "[VP] [WORDS]     :  [Word(\"'s/VBZ\")]\n",
      "[VP] [HEAD]      :  's\n",
      "Object [WORDS]   :  [Word('a/DT'), Word('shame/NN')]\n",
      "Object [HEAD]    :  shame\n",
      "Extracted Nouns  : {'shame'}\n",
      "--------------------------------------------------------------------------------\n",
      "Subject          :  None\n",
      "[VP]             :  Send\n",
      "[VP] [WORDS]     :  [Word('Send/VB')]\n",
      "[VP] [HEAD]      :  Send\n",
      "Object [WORDS]   :  [Word('an/DT'), Word('Amazon/NNP'), Word('gift/NN'), Word('card/NN')]\n",
      "Object [HEAD]    :  Amazon\n",
      "Extracted Nouns  : {'card', 'Amazon', 'gift'}\n",
      "--------------------------------------------------------------------------------\n",
      "Subject          :  I\n",
      "[VP]             :  can fix\n",
      "[VP] [WORDS]     :  [Word('can/MD'), Word('fix/VB')]\n",
      "[VP] [HEAD]      :  fix\n",
      "Object [WORDS]   :  [Word('this/DT'), Word('today/NN')]\n",
      "Object [HEAD]    :  today\n",
      "Extracted Nouns  : {'today'}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "find_ado(data[1]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I need more tiaras. I haven't bought a new one in over a year and that's a shame. Send an Amazon gift card so I can fix this today! goddessjoules@gmail.com\""
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[1]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc6 = \"Can you please recommend a good place to have lunch in Bangalore?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "b6 = TextBlob(doc6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Can', 'MD'),\n",
       " ('you', 'PRP'),\n",
       " ('please', 'VB'),\n",
       " ('recommend', 'VB'),\n",
       " ('some', 'DT'),\n",
       " ('good', 'JJ'),\n",
       " ('place', 'NN'),\n",
       " ('to', 'TO'),\n",
       " ('have', 'VB'),\n",
       " ('lunch', 'NN'),\n",
       " ('in', 'IN'),\n",
       " ('Bangalore', 'NNP')]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b6.tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject          :  None\n",
      "[VP]             :  Can\n",
      "[VP] [WORDS]     :  [Word('Can/MD')]\n",
      "Object [WORDS]   :  [Word('you/PRP')]\n",
      "Extracted Nouns  : set()\n",
      "--------------------------------------------------------------------------------\n",
      "Subject          :  you\n",
      "[VP]             :  please recommend\n",
      "[VP] [WORDS]     :  [Word('please/VB'), Word('recommend/VB')]\n",
      "Object [WORDS]   :  [Word('some/DT'), Word('good/JJ'), Word('place/NN')]\n",
      "Extracted Nouns  : {'place'}\n",
      "--------------------------------------------------------------------------------\n",
      "Subject          :  some good place\n",
      "[VP]             :  to have\n",
      "[VP] [WORDS]     :  [Word('to/TO'), Word('have/VB')]\n",
      "Object [WORDS]   :  [Word('lunch/NN')]\n",
      "Extracted Nouns  : {'lunch'}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "find_ado(doc6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TYPE     :  VP\n",
      "PNP      :  None\n",
      "STRING   :  Can\n",
      "WORDS    :  [Word('Can/MD')]\n",
      "--------------------------------------------------------------------------------\n",
      "TYPE     :  NP\n",
      "PNP      :  None\n",
      "STRING   :  you\n",
      "WORDS    :  [Word('you/PRP')]\n",
      "--------------------------------------------------------------------------------\n",
      "TYPE     :  VP\n",
      "PNP      :  None\n",
      "STRING   :  please recommend\n",
      "WORDS    :  [Word('please/VB'), Word('recommend/VB')]\n",
      "--------------------------------------------------------------------------------\n",
      "TYPE     :  NP\n",
      "PNP      :  None\n",
      "STRING   :  some good place\n",
      "WORDS    :  [Word('some/DT'), Word('good/JJ'), Word('place/NN')]\n",
      "--------------------------------------------------------------------------------\n",
      "TYPE     :  VP\n",
      "PNP      :  None\n",
      "STRING   :  to have\n",
      "WORDS    :  [Word('to/TO'), Word('have/VB')]\n",
      "--------------------------------------------------------------------------------\n",
      "TYPE     :  NP\n",
      "PNP      :  None\n",
      "STRING   :  lunch\n",
      "WORDS    :  [Word('lunch/NN')]\n",
      "--------------------------------------------------------------------------------\n",
      "TYPE     :  PP\n",
      "PNP      :  in Bangalore\n",
      "STRING   :  in\n",
      "WORDS    :  [Word('in/IN')]\n",
      "--------------------------------------------------------------------------------\n",
      "TYPE     :  NP\n",
      "PNP      :  in Bangalore\n",
      "STRING   :  Bangalore\n",
      "WORDS    :  [Word('Bangalore/NNP')]\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "s = parsetree(doc6, relations=True, lemmata=True)\n",
    "for sent in s:\n",
    "    for chunk in sent.chunks:\n",
    "        print('TYPE     : ', chunk.type)\n",
    "        print('PNP      : ', chunk.pnp)\n",
    "        print(\"STRING   : \", chunk.string)\n",
    "        print(\"WORDS    : \", chunk.words)       \n",
    "        print('-'*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TYPE     :  NP\n",
      "PNP      :  None\n",
      "STRING   :  I\n",
      "WORDS    :  [Word('I/PRP')]\n",
      "--------------------------------------------------------------------------------\n",
      "TYPE     :  VP\n",
      "PNP      :  None\n",
      "STRING   :  want to purchase\n",
      "WORDS    :  [Word('want/VBP'), Word('to/TO'), Word('purchase/VB')]\n",
      "--------------------------------------------------------------------------------\n",
      "TYPE     :  NP\n",
      "PNP      :  None\n",
      "STRING   :  a new phone\n",
      "WORDS    :  [Word('a/DT'), Word('new/JJ'), Word('phone/NN')]\n",
      "--------------------------------------------------------------------------------\n",
      "TYPE     :  VP\n",
      "PNP      :  None\n",
      "STRING   :  should i go\n",
      "WORDS    :  [Word('should/MD'), Word('i/VB'), Word('go/VB')]\n",
      "--------------------------------------------------------------------------------\n",
      "TYPE     :  PP\n",
      "PNP      :  for the iphone x\n",
      "STRING   :  for\n",
      "WORDS    :  [Word('for/IN')]\n",
      "--------------------------------------------------------------------------------\n",
      "TYPE     :  NP\n",
      "PNP      :  for the iphone x\n",
      "STRING   :  the iphone x\n",
      "WORDS    :  [Word('the/DT'), Word('iphone/NNP'), Word('x/NN')]\n",
      "--------------------------------------------------------------------------------\n",
      "TYPE     :  NP\n",
      "PNP      :  None\n",
      "STRING   :  google pixel\n",
      "WORDS    :  [Word('google/NNP-ORG'), Word('pixel/NN')]\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "s = parsetree(doc5, relations=True, lemmata=True)\n",
    "for sent in s:\n",
    "    for chunk in sent.chunks:\n",
    "        print('TYPE     : ', chunk.type)\n",
    "        print('PNP      : ', chunk.pnp)\n",
    "        print(\"STRING   : \", chunk.string)\n",
    "        print(\"WORDS    : \", chunk.words)       \n",
    "        print('-'*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_psd(doc):\n",
    "    s = parsetree(doc, relations=True, lemmata=True)\n",
    "    for sent in s:\n",
    "        for chunk in sent.chunks:\n",
    "            print('TYPE     : ', chunk.type)\n",
    "            print('HEAD     : ', chunk.head)\n",
    "            print('PNP      : ', chunk.pnp)\n",
    "            print(\"STRING   : \", chunk.string)\n",
    "            print(\"WORDS    : \", chunk.words)       \n",
    "            print('-'*80)\n",
    "        print(sent.chunks)\n",
    "        print('-'*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TYPE     :  VP\n",
      "HEAD     :  Can\n",
      "PNP      :  None\n",
      "STRING   :  Can\n",
      "WORDS    :  [Word('Can/MD')]\n",
      "--------------------------------------------------------------------------------\n",
      "TYPE     :  NP\n",
      "HEAD     :  you\n",
      "PNP      :  None\n",
      "STRING   :  you\n",
      "WORDS    :  [Word('you/PRP')]\n",
      "--------------------------------------------------------------------------------\n",
      "TYPE     :  VP\n",
      "HEAD     :  recommend\n",
      "PNP      :  None\n",
      "STRING   :  please recommend\n",
      "WORDS    :  [Word('please/VB'), Word('recommend/VB')]\n",
      "--------------------------------------------------------------------------------\n",
      "TYPE     :  NP\n",
      "HEAD     :  place\n",
      "PNP      :  None\n",
      "STRING   :  a good place\n",
      "WORDS    :  [Word('a/DT'), Word('good/JJ'), Word('place/NN')]\n",
      "--------------------------------------------------------------------------------\n",
      "TYPE     :  VP\n",
      "HEAD     :  have\n",
      "PNP      :  None\n",
      "STRING   :  to have\n",
      "WORDS    :  [Word('to/TO'), Word('have/VB')]\n",
      "--------------------------------------------------------------------------------\n",
      "TYPE     :  NP\n",
      "HEAD     :  lunch\n",
      "PNP      :  None\n",
      "STRING   :  lunch\n",
      "WORDS    :  [Word('lunch/NN')]\n",
      "--------------------------------------------------------------------------------\n",
      "TYPE     :  PP\n",
      "HEAD     :  in\n",
      "PNP      :  in Bangalore\n",
      "STRING   :  in\n",
      "WORDS    :  [Word('in/IN')]\n",
      "--------------------------------------------------------------------------------\n",
      "TYPE     :  NP\n",
      "HEAD     :  Bangalore\n",
      "PNP      :  in Bangalore\n",
      "STRING   :  Bangalore\n",
      "WORDS    :  [Word('Bangalore/NNP')]\n",
      "--------------------------------------------------------------------------------\n",
      "[Chunk('Can/VP-1'), Chunk('you/NP-OBJ-1'), Chunk('please recommend/VP-2'), Chunk('a good place/NP-OBJ-2'), Chunk('to have/VP-3'), Chunk('lunch/NP-OBJ-3'), Chunk('in/PP'), Chunk('Bangalore/NP')]\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "extract_psd(doc6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TYPE     :  NP\n",
      "HEAD     :  I\n",
      "PNP      :  None\n",
      "STRING   :  I\n",
      "WORDS    :  [Word('I/PRP')]\n",
      "--------------------------------------------------------------------------------\n",
      "TYPE     :  VP\n",
      "HEAD     :  purchase\n",
      "PNP      :  None\n",
      "STRING   :  want to purchase\n",
      "WORDS    :  [Word('want/VBP'), Word('to/TO'), Word('purchase/VB')]\n",
      "--------------------------------------------------------------------------------\n",
      "TYPE     :  NP\n",
      "HEAD     :  phone\n",
      "PNP      :  None\n",
      "STRING   :  a new phone\n",
      "WORDS    :  [Word('a/DT'), Word('new/JJ'), Word('phone/NN')]\n",
      "--------------------------------------------------------------------------------\n",
      "TYPE     :  VP\n",
      "HEAD     :  go\n",
      "PNP      :  None\n",
      "STRING   :  should i go\n",
      "WORDS    :  [Word('should/MD'), Word('i/VB'), Word('go/VB')]\n",
      "--------------------------------------------------------------------------------\n",
      "TYPE     :  PP\n",
      "HEAD     :  for\n",
      "PNP      :  for the iphone x\n",
      "STRING   :  for\n",
      "WORDS    :  [Word('for/IN')]\n",
      "--------------------------------------------------------------------------------\n",
      "TYPE     :  NP\n",
      "HEAD     :  iphone\n",
      "PNP      :  for the iphone x\n",
      "STRING   :  the iphone x\n",
      "WORDS    :  [Word('the/DT'), Word('iphone/NNP'), Word('x/NN')]\n",
      "--------------------------------------------------------------------------------\n",
      "TYPE     :  NP\n",
      "HEAD     :  google\n",
      "PNP      :  None\n",
      "STRING   :  google pixel\n",
      "WORDS    :  [Word('google/NNP-ORG'), Word('pixel/NN')]\n",
      "--------------------------------------------------------------------------------\n",
      "[Chunk('I/NP-SBJ-1'), Chunk('want to purchase/VP-1'), Chunk('a new phone/NP-OBJ-1'), Chunk('should i go/VP'), Chunk('for/PP'), Chunk('the iphone x/NP'), Chunk('google pixel/NP')]\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "extract_psd(doc5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc3 = data[2]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc4 = data[3]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2 = data[1]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = data[0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TYPE     :  ADVP\n",
      "PNP      :  None\n",
      "STRING   :  recently\n",
      "WORDS    :  [Word('recently/RB')]\n",
      "--------------------------------------------------------------------------------\n",
      "TYPE     :  NP\n",
      "PNP      :  None\n",
      "STRING   :  I\n",
      "WORDS    :  [Word('I/PRP')]\n",
      "--------------------------------------------------------------------------------\n",
      "TYPE     :  VP\n",
      "PNP      :  None\n",
      "STRING   :  was wondering\n",
      "WORDS    :  [Word('was/VBD'), Word('wondering/VBG')]\n",
      "--------------------------------------------------------------------------------\n",
      "TYPE     :  PP\n",
      "PNP      :  if it\n",
      "STRING   :  if\n",
      "WORDS    :  [Word('if/IN')]\n",
      "--------------------------------------------------------------------------------\n",
      "TYPE     :  NP\n",
      "PNP      :  if it\n",
      "STRING   :  it\n",
      "WORDS    :  [Word('it/PRP')]\n",
      "--------------------------------------------------------------------------------\n",
      "TYPE     :  VP\n",
      "PNP      :  None\n",
      "STRING   :  pays to buy\n",
      "WORDS    :  [Word('pays/VBZ'), Word('to/TO'), Word('buy/VB')]\n",
      "--------------------------------------------------------------------------------\n",
      "TYPE     :  NP\n",
      "PNP      :  None\n",
      "STRING   :  gtx 1070 or wait\n",
      "WORDS    :  [Word('gtx/NN'), Word('1070/CD'), Word('or/CC'), Word('wait/NN')]\n",
      "--------------------------------------------------------------------------------\n",
      "TYPE     :  PP\n",
      "PNP      :  None\n",
      "STRING   :  for\n",
      "WORDS    :  [Word('for/IN')]\n",
      "--------------------------------------------------------------------------------\n",
      "TYPE     :  ADJP\n",
      "PNP      :  None\n",
      "STRING   :  new\n",
      "WORDS    :  [Word('new/JJ')]\n",
      "--------------------------------------------------------------------------------\n",
      "TYPE     :  NP\n",
      "PNP      :  None\n",
      "STRING   :  this year\n",
      "WORDS    :  [Word('this/DT'), Word('year/NN')]\n",
      "--------------------------------------------------------------------------------\n",
      "TYPE     :  NP\n",
      "PNP      :  None\n",
      "STRING   :  nvidia graphics cards\n",
      "WORDS    :  [Word('nvidia/NN'), Word('graphics/NNS'), Word('cards/NNS')]\n",
      "--------------------------------------------------------------------------------\n",
      "[Chunk('recently/ADVP'), Chunk('I/NP-SBJ-1'), Chunk('was wondering/VP-1'), Chunk('if/PP'), Chunk('it/NP-SBJ-2'), Chunk('pays to buy/VP-2'), Chunk('gtx 1070 or wait/NP-OBJ-2'), Chunk('for/PP'), Chunk('new/ADJP'), Chunk('this year/NP'), Chunk('nvidia graphics cards/NP')]\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "extract_psd(doc3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hi, recently I was wondering if it pays to buy gtx 1070 or wait for new this year's nvidia graphics cards\""
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TYPE     :  NP\n",
      "PNP      :  None\n",
      "STRING   :  I\n",
      "WORDS    :  [Word('I/PRP')]\n",
      "--------------------------------------------------------------------------------\n",
      "TYPE     :  VP\n",
      "PNP      :  None\n",
      "STRING   :  want to buy\n",
      "WORDS    :  [Word('want/VBP'), Word('to/TO'), Word('buy/VB')]\n",
      "--------------------------------------------------------------------------------\n",
      "TYPE     :  NP\n",
      "PNP      :  None\n",
      "STRING   :  a phone\n",
      "WORDS    :  [Word('a/DT'), Word('phone/NN')]\n",
      "--------------------------------------------------------------------------------\n",
      "[Chunk('I/NP-SBJ-1'), Chunk('want to buy/VP-1'), Chunk('a phone/NP-OBJ-1')]\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "extract_psd(\"I want to buy a phone\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TYPE     :  VP\n",
      "PNP      :  None\n",
      "STRING   :  Can\n",
      "OBJECT   :  you\n",
      "--------------------------------------------------------------------------------\n",
      "TYPE     :  NP\n",
      "PNP      :  None\n",
      "STRING   :  you\n",
      "WORDS    :  [Word('you/PRP')]\n",
      "OBJECT   :  None\n",
      "--------------------------------------------------------------------------------\n",
      "TYPE     :  VP\n",
      "PNP      :  None\n",
      "STRING   :  please recommend\n",
      "OBJECT   :  some good place\n",
      "--------------------------------------------------------------------------------\n",
      "TYPE     :  NP\n",
      "PNP      :  None\n",
      "STRING   :  some good place\n",
      "WORDS    :  [Word('some/DT'), Word('good/JJ'), Word('place/NN')]\n",
      "OBJECT   :  None\n",
      "--------------------------------------------------------------------------------\n",
      "TYPE     :  VP\n",
      "PNP      :  None\n",
      "STRING   :  to have\n",
      "OBJECT   :  lunch\n",
      "--------------------------------------------------------------------------------\n",
      "TYPE     :  NP\n",
      "PNP      :  None\n",
      "STRING   :  lunch\n",
      "WORDS    :  [Word('lunch/NN')]\n",
      "OBJECT   :  None\n",
      "--------------------------------------------------------------------------------\n",
      "TYPE     :  PP\n",
      "PNP      :  in Bangalore\n",
      "STRING   :  in\n",
      "WORDS    :  [Word('in/IN')]\n",
      "OBJECT   :  None\n",
      "--------------------------------------------------------------------------------\n",
      "TYPE     :  NP\n",
      "PNP      :  in Bangalore\n",
      "STRING   :  Bangalore\n",
      "WORDS    :  [Word('Bangalore/NNP')]\n",
      "OBJECT   :  None\n",
      "--------------------------------------------------------------------------------\n",
      "[Chunk('Can/VP-1'), Chunk('you/NP-OBJ-1'), Chunk('please recommend/VP-2'), Chunk('some good place/NP-OBJ-2'), Chunk('to have/VP-3'), Chunk('lunch/NP-OBJ-3'), Chunk('in/PP'), Chunk('Bangalore/NP')]\n"
     ]
    }
   ],
   "source": [
    "s = parsetree(doc6, relations=True, lemmata=True)\n",
    "for sent in s:\n",
    "    for chunk in sent.chunks:\n",
    "        if chunk.type == 'VP' and chunk.object is not None:\n",
    "            print(\"TYPE     : \", chunk.type)            \n",
    "            print(\"PNP      : \", chunk.pnp)\n",
    "            print(\"STRING   : \", chunk.string)            \n",
    "            print(\"OBJECT   : \", chunk.object)            \n",
    "            print('-'*80)            \n",
    "        if chunk.type == \"PP\" or chunk.type == \"NP\":\n",
    "            print(\"TYPE     : \", chunk.type)            \n",
    "            print(\"PNP      : \", chunk.pnp)\n",
    "            print(\"STRING   : \", chunk.string)\n",
    "            print(\"WORDS    : \", chunk.words)\n",
    "            print(\"OBJECT   : \", chunk.object)\n",
    "            print('-'*80)\n",
    "    print(sent.chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TYPE     :  VP\n",
      "PNP      :  None\n",
      "STRING   :  are\n",
      "OBJECT   :  some good places\n",
      "OBJECT [Nearest PP]: in\n",
      "OBJECT [Next NP]: Los Angeles\n",
      "--------------------------------------------------------------------------------\n",
      "TYPE     :  VP\n",
      "PNP      :  None\n",
      "STRING   :  to have\n",
      "OBJECT   :  lunch\n",
      "OBJECT [Nearest PP]: in\n",
      "OBJECT [Next NP]: Los Angeles\n",
      "--------------------------------------------------------------------------------\n",
      "[Chunk('are/VP-1'), Chunk('some good places/NP-OBJ-1'), Chunk('to have/VP-2'), Chunk('lunch/NP-OBJ-2'), Chunk('in/PP'), Chunk('Los Angeles/NP'), Chunk('for/PP'), Chunk('dinner/NP')]\n"
     ]
    }
   ],
   "source": [
    "s = parsetree(\"What are some good places to have lunch in Los Angeles for dinner?\", relations=True, lemmata=True)\n",
    "for sent in s:\n",
    "    for chunk in sent.chunks:\n",
    "        if chunk.type == 'VP' and chunk.object is not None:\n",
    "            print(\"TYPE     : \", chunk.type)            \n",
    "            print(\"PNP      : \", chunk.pnp)\n",
    "            print(\"STRING   : \", chunk.string)            \n",
    "            print(\"OBJECT   : \", chunk.object)\n",
    "            nearest_pp = chunk.object.nearest('PP')\n",
    "            print(\"OBJECT [Nearest PP]:\", nearest_pp)\n",
    "            print(\"OBJECT [Next NP]:\", nearest_pp.next(type='NP'))\n",
    "            print('-'*80)            \n",
    "#         if chunk.type == \"PP\" or chunk.type == \"NP\":\n",
    "#             print(\"TYPE     : \", chunk.type)            \n",
    "#             print(\"PNP      : \", chunk.pnp)\n",
    "#             print(\"STRING   : \", chunk.string)\n",
    "#             print(\"WORDS    : \", chunk.words)\n",
    "#             print(\"OBJECT   : \", chunk.object)\n",
    "#             print('-'*80)\n",
    "    print(sent.chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/app/bin/nltk_data/stanford-ner/classifiers/english.all.3class.distsim.crf.ser.gz'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.join(NLTK_PATH, 'stanford-ner/classifiers/english.all.3class.distsim.crf.ser.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/app/bin/nltk_data/stanford-ner/stanford-ner.jar'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.join(NLTK_PATH, 'stanford-ner/stanford-ner.jar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/app/playground/notebooks'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/nltk/tag/stanford.py:183: DeprecationWarning: \n",
      "The StanfordTokenizer will be deprecated in version 3.2.5.\n",
      "Please use \u001b[91mnltk.tag.corenlp.CoreNLPPOSTagger\u001b[0m or \u001b[91mnltk.tag.corenlp.CoreNLPNERTagger\u001b[0m instead.\n",
      "  super(StanfordNERTagger, self).__init__(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "from nltk.tag import StanfordNERTagger\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "NLTK_PATH = os.environ.get('NLTK_DATA', None)\n",
    "\n",
    "tagger = StanfordNERTagger(\n",
    "    os.path.join(NLTK_PATH, 'stanford-ner/classifiers/english.all.3class.distsim.crf.ser.gz'),\n",
    "    os.path.join(NLTK_PATH, 'stanford-ner/stanford-ner.jar'),\n",
    "    encoding='utf-8'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_doc6 = word_tokenize(doc6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Can you please recommend a good place to have lunch in Bangalore?'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Can', 'O'),\n",
       " ('you', 'O'),\n",
       " ('please', 'O'),\n",
       " ('recommend', 'O'),\n",
       " ('a', 'O'),\n",
       " ('good', 'O'),\n",
       " ('place', 'O'),\n",
       " ('to', 'O'),\n",
       " ('have', 'O'),\n",
       " ('lunch', 'O'),\n",
       " ('in', 'O'),\n",
       " ('Bangalore?', 'O')]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger.tag(doc6.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_ = [ word_tokenize(word) for word in doc6.split() ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Can'],\n",
       " ['you'],\n",
       " ['please'],\n",
       " ['recommend'],\n",
       " ['a'],\n",
       " ['good'],\n",
       " ['place'],\n",
       " ['to'],\n",
       " ['have'],\n",
       " ['lunch'],\n",
       " ['in'],\n",
       " ['Bangalore', '?']]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('Can', 'O')],\n",
       " [('you', 'O')],\n",
       " [('please', 'O')],\n",
       " [('recommend', 'O')],\n",
       " [('a', 'O')],\n",
       " [('good', 'O')],\n",
       " [('place', 'O')],\n",
       " [('to', 'O')],\n",
       " [('have', 'O')],\n",
       " [('lunch', 'O')],\n",
       " [('in', 'O')],\n",
       " [('Bangalore', 'LOCATION'), ('?', 'O')]]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger.tag_sents(tokenized_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "btokenized_ = [ word_tokenize(\"Bangalore\") ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('Bangalore', 'LOCATION')]]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger.tag_sents(btokenized_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Bangalore', 'LOCATION')]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger.tag(word_tokenize('Bangalore'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _extract_ado(document):\n",
    "    \"\"\"\n",
    "        PA word has dependent object of PO category (ADO)\n",
    "\n",
    "        In a PI post, a purchase action is targeted towards a consumable object. \n",
    "        This is reflected in the dependency structure of the text.\n",
    "\n",
    "        In a PI post, the consumable object is usually the directly \n",
    "        dependent object of the purchase action verb.\n",
    "\n",
    "        If there is a PA word in the text and it has a dependent object belonging to a PO category, ADO = 1, otherwise ADO = 0\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Identify if there is a PA word. (or a very similar one)\n",
    "    # 2. Identify if this PA word has an object.\n",
    "    # 3. Identify if this object belongs to the PO category.\n",
    "    # 4. If the 3 statements above are true, return ADO = 1 else ADO = 0\n",
    "\n",
    "    PA_WORD_PRESENT = False\n",
    "    PO_CATEGORY_PRESENT = False\n",
    "\n",
    "    s = parsetree(document, relations=True, lemmata=True)\n",
    "    for sentence in s:\n",
    "        for chunk in sentence.chunks:\n",
    "            print(\"Chunk: {} -- Type: {}, Object: {}, PNP: {}\".format(chunk, chunk.type, chunk.object, chunk.pnp))\n",
    "            # --- ado extraction --- \n",
    "            if chunk.type == 'VP' and chunk.object is not None:\n",
    "\n",
    "                obj = chunk.object\n",
    "                verbs = [w.string for w in chunk.words]\n",
    "\n",
    "                for v in verbs:\n",
    "                    PA_WORD_PRESENT = False\n",
    "                    if PA_WORD_PRESENT:\n",
    "                        break\n",
    "\n",
    "                nouns = filter(lambda x: x.type in NOUN_TAGS, obj.words)        \n",
    "                nouns = set([noun.string for noun in nouns])\n",
    "\n",
    "                for n in nouns:                        \n",
    "                    PO_CATEGORY_PRESENT = False\n",
    "                    if PO_CATEGORY_PRESENT:\n",
    "                        break\n",
    "\n",
    "                if PA_WORD_PRESENT and PO_CATEGORY_PRESENT == True:\n",
    "                    return int(PA_WORD_PRESENT and PO_CATEGORY_PRESENT)\n",
    "\n",
    "            # --- ado extraction --- \n",
    "\n",
    "\n",
    "    return int(PA_WORD_PRESENT and PO_CATEGORY_PRESENT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk: are -- Type: VP, Object: some good places, PNP: None\n",
      "Chunk: some good places -- Type: NP, Object: None, PNP: None\n",
      "Chunk: to have -- Type: VP, Object: lunch, PNP: None\n",
      "Chunk: lunch -- Type: NP, Object: None, PNP: None\n",
      "Chunk: in -- Type: PP, Object: None, PNP: in Bangalore\n",
      "Chunk: Bangalore -- Type: NP, Object: None, PNP: in Bangalore\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc7 = \"What are some good places to have lunch in Bangalore\"\n",
    "_extract_ado(doc7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hi, recently I was wondering if it pays to buy gtx 1070 or wait for new this year's nvidia graphics cards\""
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I need more tiaras. I haven't bought a new one in over a year and that's a shame. Send an Amazon gift card so I can fix this today! goddessjoules@gmail.com\""
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TYPE     :  ADVP\n",
      "PNP      :  None\n",
      "STRING   :  recently\n",
      "WORDS    :  [Word('recently/RB')]\n",
      "--------------------------------------------------------------------------------\n",
      "TYPE     :  NP\n",
      "PNP      :  None\n",
      "STRING   :  I\n",
      "WORDS    :  [Word('I/PRP')]\n",
      "--------------------------------------------------------------------------------\n",
      "TYPE     :  VP\n",
      "PNP      :  None\n",
      "STRING   :  was wondering\n",
      "WORDS    :  [Word('was/VBD'), Word('wondering/VBG')]\n",
      "--------------------------------------------------------------------------------\n",
      "TYPE     :  PP\n",
      "PNP      :  if it\n",
      "STRING   :  if\n",
      "WORDS    :  [Word('if/IN')]\n",
      "--------------------------------------------------------------------------------\n",
      "TYPE     :  NP\n",
      "PNP      :  if it\n",
      "STRING   :  it\n",
      "WORDS    :  [Word('it/PRP')]\n",
      "--------------------------------------------------------------------------------\n",
      "TYPE     :  VP\n",
      "PNP      :  None\n",
      "STRING   :  pays to buy\n",
      "WORDS    :  [Word('pays/VBZ'), Word('to/TO'), Word('buy/VB')]\n",
      "--------------------------------------------------------------------------------\n",
      "TYPE     :  NP\n",
      "PNP      :  None\n",
      "STRING   :  gtx 1070 or wait\n",
      "WORDS    :  [Word('gtx/NN'), Word('1070/CD'), Word('or/CC'), Word('wait/NN')]\n",
      "--------------------------------------------------------------------------------\n",
      "TYPE     :  PP\n",
      "PNP      :  None\n",
      "STRING   :  for\n",
      "WORDS    :  [Word('for/IN')]\n",
      "--------------------------------------------------------------------------------\n",
      "TYPE     :  ADJP\n",
      "PNP      :  None\n",
      "STRING   :  new\n",
      "WORDS    :  [Word('new/JJ')]\n",
      "--------------------------------------------------------------------------------\n",
      "TYPE     :  NP\n",
      "PNP      :  None\n",
      "STRING   :  this year\n",
      "WORDS    :  [Word('this/DT'), Word('year/NN')]\n",
      "--------------------------------------------------------------------------------\n",
      "TYPE     :  NP\n",
      "PNP      :  None\n",
      "STRING   :  nvidia graphics cards\n",
      "WORDS    :  [Word('nvidia/NN'), Word('graphics/NNS'), Word('cards/NNS')]\n",
      "--------------------------------------------------------------------------------\n",
      "[Chunk('recently/ADVP'), Chunk('I/NP-SBJ-1'), Chunk('was wondering/VP-1'), Chunk('if/PP'), Chunk('it/NP-SBJ-2'), Chunk('pays to buy/VP-2'), Chunk('gtx 1070 or wait/NP-OBJ-2'), Chunk('for/PP'), Chunk('new/ADJP'), Chunk('this year/NP'), Chunk('nvidia graphics cards/NP')]\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "extract_psd(doc3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I need more tiaras. I haven't bought a new one in over a year and that's a shame. Send an Amazon gift card so I can fix this today! goddessjoules@gmail.com\""
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TYPE     :  NP\n",
      "HEAD     :  I\n",
      "PNP      :  None\n",
      "STRING   :  I\n",
      "WORDS    :  [Word('I/PRP')]\n",
      "--------------------------------------------------------------------------------\n",
      "TYPE     :  VP\n",
      "HEAD     :  need\n",
      "PNP      :  None\n",
      "STRING   :  need\n",
      "WORDS    :  [Word('need/VBP')]\n",
      "--------------------------------------------------------------------------------\n",
      "TYPE     :  NP\n",
      "HEAD     :  tiaras\n",
      "PNP      :  None\n",
      "STRING   :  more tiaras\n",
      "WORDS    :  [Word('more/JJR'), Word('tiaras/NNS')]\n",
      "--------------------------------------------------------------------------------\n",
      "[Chunk('I/NP-SBJ-1'), Chunk('need/VP-1'), Chunk('more tiaras/NP-OBJ-1')]\n",
      "--------------------------------------------------------------------------------\n",
      "TYPE     :  NP\n",
      "HEAD     :  I\n",
      "PNP      :  None\n",
      "STRING   :  I\n",
      "WORDS    :  [Word('I/PRP')]\n",
      "--------------------------------------------------------------------------------\n",
      "TYPE     :  VP\n",
      "HEAD     :  bought\n",
      "PNP      :  None\n",
      "STRING   :  have n't bought\n",
      "WORDS    :  [Word('have/VBP'), Word(\"n't/RB\"), Word('bought/VBN')]\n",
      "--------------------------------------------------------------------------------\n",
      "TYPE     :  ADJP\n",
      "HEAD     :  new\n",
      "PNP      :  None\n",
      "STRING   :  new\n",
      "WORDS    :  [Word('new/JJ')]\n",
      "--------------------------------------------------------------------------------\n",
      "TYPE     :  PP\n",
      "HEAD     :  in\n",
      "PNP      :  in over a year\n",
      "STRING   :  in over\n",
      "WORDS    :  [Word('in/IN'), Word('over/IN')]\n",
      "--------------------------------------------------------------------------------\n",
      "TYPE     :  NP\n",
      "HEAD     :  year\n",
      "PNP      :  in over a year\n",
      "STRING   :  a year\n",
      "WORDS    :  [Word('a/DT'), Word('year/NN')]\n",
      "--------------------------------------------------------------------------------\n",
      "TYPE     :  VP\n",
      "HEAD     :  's\n",
      "PNP      :  None\n",
      "STRING   :  's\n",
      "WORDS    :  [Word(\"'s/VBZ\")]\n",
      "--------------------------------------------------------------------------------\n",
      "TYPE     :  NP\n",
      "HEAD     :  shame\n",
      "PNP      :  None\n",
      "STRING   :  a shame\n",
      "WORDS    :  [Word('a/DT'), Word('shame/NN')]\n",
      "--------------------------------------------------------------------------------\n",
      "[Chunk('I/NP-SBJ-1'), Chunk('have n't bought/VP-1'), Chunk('new/ADJP'), Chunk('in over/PP'), Chunk('a year/NP'), Chunk(''s/VP-2'), Chunk('a shame/NP-OBJ-2')]\n",
      "--------------------------------------------------------------------------------\n",
      "TYPE     :  VP\n",
      "HEAD     :  Send\n",
      "PNP      :  None\n",
      "STRING   :  Send\n",
      "WORDS    :  [Word('Send/VB')]\n",
      "--------------------------------------------------------------------------------\n",
      "TYPE     :  NP\n",
      "HEAD     :  Amazon\n",
      "PNP      :  None\n",
      "STRING   :  an Amazon gift card\n",
      "WORDS    :  [Word('an/DT'), Word('Amazon/NNP'), Word('gift/NN'), Word('card/NN')]\n",
      "--------------------------------------------------------------------------------\n",
      "TYPE     :  PP\n",
      "HEAD     :  so\n",
      "PNP      :  so I\n",
      "STRING   :  so\n",
      "WORDS    :  [Word('so/IN')]\n",
      "--------------------------------------------------------------------------------\n",
      "TYPE     :  NP\n",
      "HEAD     :  I\n",
      "PNP      :  so I\n",
      "STRING   :  I\n",
      "WORDS    :  [Word('I/PRP')]\n",
      "--------------------------------------------------------------------------------\n",
      "TYPE     :  VP\n",
      "HEAD     :  fix\n",
      "PNP      :  None\n",
      "STRING   :  can fix\n",
      "WORDS    :  [Word('can/MD'), Word('fix/VB')]\n",
      "--------------------------------------------------------------------------------\n",
      "TYPE     :  NP\n",
      "HEAD     :  today\n",
      "PNP      :  None\n",
      "STRING   :  this today\n",
      "WORDS    :  [Word('this/DT'), Word('today/NN')]\n",
      "--------------------------------------------------------------------------------\n",
      "[Chunk('Send/VP-1'), Chunk('an Amazon gift card/NP-OBJ-1'), Chunk('so/PP'), Chunk('I/NP-SBJ-2'), Chunk('can fix/VP-2'), Chunk('this today/NP-OBJ-2')]\n",
      "--------------------------------------------------------------------------------\n",
      "TYPE     :  NP\n",
      "HEAD     :  goddessjoules@gmail.com\n",
      "PNP      :  None\n",
      "STRING   :  goddessjoules@gmail.com\n",
      "WORDS    :  [Word('goddessjoules@gmail.com/NNP')]\n",
      "--------------------------------------------------------------------------------\n",
      "[Chunk('goddessjoules@gmail.com/NP')]\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "extract_psd(doc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TYPE     :  VP\n",
      "HEAD     :  are\n",
      "PNP      :  None\n",
      "STRING   :  are\n",
      "WORDS    :  [Word('are/VBP')]\n",
      "--------------------------------------------------------------------------------\n",
      "TYPE     :  NP\n",
      "HEAD     :  places\n",
      "PNP      :  None\n",
      "STRING   :  some good places\n",
      "WORDS    :  [Word('some/DT'), Word('good/JJ'), Word('places/NNS')]\n",
      "--------------------------------------------------------------------------------\n",
      "TYPE     :  VP\n",
      "HEAD     :  have\n",
      "PNP      :  None\n",
      "STRING   :  to have\n",
      "WORDS    :  [Word('to/TO'), Word('have/VB')]\n",
      "--------------------------------------------------------------------------------\n",
      "TYPE     :  NP\n",
      "HEAD     :  lunch\n",
      "PNP      :  None\n",
      "STRING   :  lunch\n",
      "WORDS    :  [Word('lunch/NN')]\n",
      "--------------------------------------------------------------------------------\n",
      "TYPE     :  PP\n",
      "HEAD     :  in\n",
      "PNP      :  in Bangalore\n",
      "STRING   :  in\n",
      "WORDS    :  [Word('in/IN')]\n",
      "--------------------------------------------------------------------------------\n",
      "TYPE     :  NP\n",
      "HEAD     :  Bangalore\n",
      "PNP      :  in Bangalore\n",
      "STRING   :  Bangalore\n",
      "WORDS    :  [Word('Bangalore/NNP')]\n",
      "--------------------------------------------------------------------------------\n",
      "[Chunk('are/VP-1'), Chunk('some good places/NP-OBJ-1'), Chunk('to have/VP-2'), Chunk('lunch/NP-OBJ-2'), Chunk('in/PP'), Chunk('Bangalore/NP')]\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "extract_psd(doc7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_nearest_np(document):\n",
    "    s = parsetree(document, relations=True, lemmata=True)\n",
    "    for sentence in s:\n",
    "        for chunk in sentence.chunks:\n",
    "\n",
    "            if chunk.type == 'VP' and chunk.object is not None:\n",
    "\n",
    "                verb = chunk.head\n",
    "                obj  = chunk.object\n",
    "                pp   = obj.nearest(type='PP')\n",
    "                if pp is not None:\n",
    "                    np = pp.next(type='NP')\n",
    "                    return np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_psd(noun_phrase):                        \n",
    "    tokenized  = [ word_tokenize(word.string) for word in noun_phrase ]\n",
    "    classified = tagger.tag_sents(tokenized)\n",
    "    flattened  = itertools.chain.from_iterable(classified)\n",
    "        \n",
    "    loc = any(filter(lambda x: x[1] == 'LOCATION', flattened))\n",
    "    org = any(filter(lambda x: x[1] == 'ORGANIZATION', flattened))\n",
    "    \n",
    "    return {\n",
    "        'LOC': loc,\n",
    "        'ORG': org\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_phrase = extract_nearest_np(toy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Chunk('a recent interview/NP')"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noun_phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "classified = classify_np(np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('this', 'O')], [('year', 'O')]]"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Bangalore', 'LOCATION')]"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(filter(lambda x: x[1] == 'LOCATION', classified[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
